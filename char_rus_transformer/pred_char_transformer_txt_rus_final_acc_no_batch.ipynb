{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pred_char_transformer_txt_rus_final_acc_no_batch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MvxMZz3nv3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dfbbde-ce96-44b7-e873-39de19594bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading lines...\n",
            "Read 402924 name pairs\n",
            "Counting characters:\n",
            "counted characters:\n",
            "En 287\n",
            "Rus 115\n",
            "Source name:  Irina Mironova\n",
            "Prediction:  ['лриоч', 'миргоБл']\n",
            "Time taken:  0m 0s\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division \n",
        "from io import open \n",
        "import string\n",
        "import re\n",
        "import unicodedata\n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "#plt.switch_backend(\"agg\")\n",
        "import matplotlib.ticker as ticker \n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "#!pip install thai_tokenizer \n",
        "#from thai_tokenizer import Tokenizer\n",
        "#from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(123456789)\n",
        "\n",
        "\n",
        "##############3\n",
        "#Functions that transform the words in numbers. Needs a tokenizer since in certain languages \n",
        "#1 english word=n other language words \n",
        "###############\n",
        "\n",
        "src_lang=\"En\"\n",
        "tgt_lang=\"Rus\"\n",
        "\n",
        "\n",
        "unk_idx=0\n",
        "pad_idx=1\n",
        "bos_idx=2\n",
        "eos_idx=3\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,emb_size:int,dropout:float,maxlen=5000):\n",
        "        super().__init__()\n",
        "        den=torch.exp(-torch.arange(0,emb_size,2)*math.log(10000)/emb_size)\n",
        "        pos=torch.arange(0,maxlen).reshape(maxlen,1)\n",
        "        pos_embedding=torch.zeros((maxlen,emb_size))\n",
        "        pos_embedding[:,0::2]=torch.sin(pos*den)\n",
        "        pos_embedding[:,1::2]=torch.cos(pos*den)\n",
        "        pos_embedding=pos_embedding.unsqueeze(-2)\n",
        "        \n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.register_buffer(\"pos_embedding\",pos_embedding)\n",
        "        \n",
        "    def forward(self,token_embedding):\n",
        "        return self.dropout(token_embedding+self.pos_embedding[:token_embedding.size(0),:])\n",
        "        \n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_size):\n",
        "        super().__init__()\n",
        "        self.embedding=nn.Embedding(vocab_size,emb_size)\n",
        "        self.emb_size=emb_size\n",
        "        \n",
        "    def forward(self,tokens):\n",
        "        return self.embedding(tokens.long())*math.sqrt(self.emb_size)\n",
        "        \n",
        "        \n",
        "class seq2seqTransformer(nn.Module):\n",
        "    def __init__(self,num_encoder_layers,num_decoder_layers,emb_size,nhead,src_vocab_size,tgt_vocab_size,\n",
        "                dim_feedforward=512,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer=nn.Transformer(d_model=emb_size,nhead=nhead,num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,dropout=dropout)\n",
        "        \n",
        "        self.generator=nn.Linear(emb_size,tgt_vocab_size)\n",
        "        self.src_tok_emb=TokenEmbedding(src_vocab_size,emb_size)\n",
        "        self.tgt_tok_emb=TokenEmbedding(tgt_vocab_size,emb_size)\n",
        "        self.positional_encoding=PositionalEncoding(emb_size,dropout)\n",
        "        \n",
        "    def forward(self,src,tgt,src_mask,tgt_mask,src_padding_mask,tgt_padding_mask,memory_key_padding_mask):\n",
        "        src_emb=self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb=self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        out=self.transformer(src_emb,tgt_emb,src_mask,tgt_mask,None,src_padding_mask,tgt_padding_mask,memory_key_padding_mask) \n",
        "        out=self.generator(out)\n",
        "        return out\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)),src_mask)\n",
        "    \n",
        "    def decode(self,tgt,memory,tgt_mask):\n",
        "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)),memory,tgt_mask)\n",
        "        \n",
        "\n",
        "def generate_square_mask(size):\n",
        "    mask=(torch.triu(torch.ones((size,size),device=device))==1).transpose(0,1)   #make lower triangular with 1's\n",
        "    #make suare matrix in upper triangle has -inf and in the lower triangle it has 0.0\n",
        "    mask=mask.float().masked_fill(mask==0,float(\"-inf\")).masked_fill(mask==1,float(0.0))   \n",
        "    return mask\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self,lang):\n",
        "        self.lang=lang\n",
        "        self.char2index={}\n",
        "        self.char2count={}\n",
        "        self.index2char={0: \"<unk>\", 1:\"<pad>\", 2:\"<bos>\", 3:\"<eos>\" }\n",
        "        self.n_chars=4\n",
        "        \n",
        "    def addName(self,sentence):\n",
        "        for char in sentence:\n",
        "            self.addChar(char)\n",
        "            \n",
        "    def addChar(self, char):\n",
        "        if char not in self.char2index:\n",
        "            self.char2index[char]=self.n_chars\n",
        "            self.char2count[char]=1\n",
        "            self.index2char[self.n_chars]=char\n",
        "            self.n_chars+=1\n",
        "        else:\n",
        "            self.char2count[char]+=1 \n",
        "            \n",
        "token_transform = {}\n",
        "def timming(since):\n",
        "    now=time.time()\n",
        "    s=now-since\n",
        "    m=int(s/60)\n",
        "    s=s-m*60\n",
        "    return \"%dm %ds\"%(m,s)\n",
        "def unicodeToAscii(s):\n",
        "        return ''.join(c for c in unicodedata.normalize(\"NFD\",s) if unicodedata.category(c)!=\"Mn\")\n",
        "def normalizeString(string):\n",
        "    string=unicodeToAscii(string.lower().strip())\n",
        "    string=re.sub(r\"([.!?])\",r\" \\1\",string)\n",
        "    string=string.replace(\",\",\"\")\n",
        "    return string\n",
        "\n",
        "def readLangs(src_lang,tgt_lang,reverse):\n",
        "    \n",
        "    forbidden_chars=['\\u200e','\\u200b','\\u200f','ª',')','|','[',']','ǃ','$','‐','?','+','̱','̄','̜','̕','︡', '︠','̈',\"ʿ\",'\"',\"ʹ\",\"ʼ\",\"‘\",\"′\",\"'́'\",\"«\",\"»\",\"\\xad\",\"\\\\\"]\n",
        "    apostrophes=[\"`\",\"’\",\"ʻ\",\"ʾ\",\"'\"]\n",
        "    chirilic=[\"А\", \"Б\", \"В\", \"Г\", \"Д\", \"Е\", \"Ё\", \"Ж\", \"З\", \"И\", \"Й\", \"К\", \"Л\", \"М\", \"Н\", \"О\", \"П\", \"Р\", \"С\", \"Т\", \"У\", \"Ф\", \"Х\", \"Ц\", \"Ч\", \"Ш\", \"Щ\", \"Ъ\", \n",
        "              \"Ы\", \"Ь\", \"Э\", \"Ю\", \"Я\"]\n",
        "    chirilic=[char.lower() for char in chirilic]\n",
        "    sh=[\"ş\",\"ṣ\",\"ș\"]\n",
        "\n",
        "    print(\"reading lines...\")\n",
        "    \n",
        "    #lines=open(\"/content/drive/MyDrive/WunderSchildt/Transformer/Data/%s-%s Names.txt\"%(src_lang,tgt_lang),encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
        "    lines=open(\"/content/drive/MyDrive/WunderSchildt/Translation/Data/%s-%s Names.txt\"%(src_lang,tgt_lang),encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
        "    pairs=[]\n",
        "    i=0\n",
        "    while i<=len(lines)-1:\n",
        "      flag=1\n",
        "      #Keep turkish, etc names or not?\n",
        "      #lines[i]=unicodeToAscii(lines[i])\n",
        "      if \"-\" in lines[i]:\n",
        "        lines[i]=lines[i].replace(\"-\",\"-\")\n",
        "      if \"–\" in lines[i]:\n",
        "        lines[i]=lines[i].replace(\"–\",\"-\")\n",
        "      for forbidden_char in forbidden_chars:\n",
        "        if forbidden_char in lines[i]:\n",
        "          lines[i]=lines[i].replace(forbidden_char,\"\")\n",
        "      for apos in apostrophes:\n",
        "        if apos in lines[i]:\n",
        "          lines[i]=lines[i].replace(apos,\"'\") \n",
        "      for s in sh:\n",
        "        if s in lines[i]:\n",
        "          lines[i]=lines[i].replace(s,\"ş\")\n",
        "      \n",
        "      if reverse==False:\n",
        "        s_input=lines[i].split(\"\\t\")[0]\n",
        "        s_output=lines[i].split(\"\\t\")[1]\n",
        "        temp_output=s_output.replace(\" \",\"\").replace(\",\",\"\").replace(\"-\",\"\")\n",
        "        temp_output=[char.lower() for char in temp_output]\n",
        "        for char in temp_output:\n",
        "          if char not in chirilic:\n",
        "            flag=0\n",
        "            break\n",
        "      else:\n",
        "        s_output=lines[i].split(\"\\t\")[0]\n",
        "        s_input=lines[i].split(\"\\t\")[1]\n",
        "        temp_input=s_input.replace(\" \",\"\").replace(\",\",\"\").replace(\"-\",\"\")\n",
        "        temp_input=[char.lower() for char in temp_input]\n",
        "        for char in temp_input:\n",
        "          if char not in chirilic:\n",
        "            flag=0\n",
        "            break\n",
        "\n",
        "      if flag==0:\n",
        "        i+=1\n",
        "      else:\n",
        "        if \",\" in lines[i]:\n",
        "          if \",\" in s_output:\n",
        "            temp=s_output.split(\",\")\n",
        "            temp=[word.replace(\" \",\"\") for word in temp]\n",
        "            temp=[temp[len(temp)-i] for i in range(1,len(temp)+1)]\n",
        "            s_output=\" \".join(temp)\n",
        "            s_output=[char.lower() for char in s_output]\n",
        "          if \",\" in s_input:\n",
        "            temp=s_input.split(\",\")\n",
        "            temp=[word.replace(\" \",\"\") for word in temp]\n",
        "            temp=[temp[len(temp)-i] for i in range(1,len(temp)+1)]\n",
        "            s_input=\" \".join(temp) \n",
        "            s_input=[char.lower() for char in s_input]    \n",
        "          pairs.append([s_input,s_output])\n",
        "        else: \n",
        "          s_input=[char.lower() for char in s_input]\n",
        "          s_output=[char.lower() for char in s_output]\n",
        "          pairs.append([s_input,s_output])\n",
        "        i+=1\n",
        "\n",
        "    if reverse:\n",
        "        #pairs=[list(reversed(p)) for p in pairs]\n",
        "        input_lang=Lang(tgt_lang)\n",
        "        output_lang=Lang(src_lang)\n",
        "    else:\n",
        "        input_lang=Lang(src_lang)\n",
        "        output_lang=Lang(tgt_lang) \n",
        "        \n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def prepareData(src_lang,tgt_lang,reverse):\n",
        "    input_lang, output_lang, pairs=readLangs(src_lang,tgt_lang,reverse)\n",
        "    #token_transform[src_lang] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "    #token_transform[tgt_lang] = get_tokenizer('spacy', language='rus')\n",
        "    print(\"Read %s name pairs\"%len(pairs))\n",
        "    print(\"Counting characters:\")\n",
        "    for pair in pairs:      \n",
        "        input_lang.addName(pair[0])\n",
        "        output_lang.addName(pair[1]) \n",
        "        \n",
        "    print(\"counted characters:\")\n",
        "    print(input_lang.lang,input_lang.n_chars) \n",
        "    print(output_lang.lang,output_lang.n_chars) \n",
        "    \n",
        "    return input_lang,output_lang, pairs\n",
        "\n",
        "\n",
        "def prepareData(src_lang,tgt_lang,reverse=False):\n",
        "    input_lang, output_lang, pairs=readLangs(src_lang,tgt_lang,reverse=False)\n",
        "    #token_transform[src_lang] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "    #token_transform[tgt_lang] = get_tokenizer('spacy', language='rus')\n",
        "    print(\"Read %s name pairs\"%len(pairs))\n",
        "    print(\"Counting characters:\")\n",
        "    for pair in pairs:      \n",
        "        input_lang.addName(pair[0])\n",
        "        output_lang.addName(pair[1])\n",
        "        \n",
        "    print(\"counted characters:\")\n",
        "    print(input_lang.lang,input_lang.n_chars)\n",
        "    print(output_lang.lang,output_lang.n_chars) \n",
        "    \n",
        "    return input_lang,output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs=prepareData(src_lang,tgt_lang,reverse=True)\n",
        "\n",
        "\n",
        "transformer=torch.load(\"/content/drive/MyDrive/WunderSchildt/Translation/Saved_models/Char transformer, rus, names 50000,embedding 512, loss 4.0000, layers 0, acc 39.69%.pth\",\n",
        "                       map_location=torch.device('cpu'))\n",
        "\n",
        "src_vocab_size=input_lang.n_chars\n",
        "tgt_vocab_size=output_lang.n_chars\n",
        "emb_size=512\n",
        "nhead=8\n",
        "#ffn_hid_dim=256\n",
        "batch_size=32\n",
        "num_encoder_layers=4\n",
        "num_decoder_layers=4\n",
        "\n",
        "\n",
        "start=time.time()\n",
        "src_name=\"Irina Mironova\"\n",
        "#tgt_name=pairs_test[0][1]\n",
        "src_list=[input_lang.char2index[char] for char in src_name]\n",
        "src_list.insert(0,bos_idx)\n",
        "src_list.append(eos_idx)\n",
        "src=torch.tensor(src_list,dtype=torch.long,device=device).view(-1,1)\n",
        "src=src.view(src.shape[0],1,1)\n",
        "#src=pad_sequence(src,padding_value=pad_idx)\n",
        "\n",
        "src=src[:,0].to(device)\n",
        "src_seq_len=src.shape[0]\n",
        "src_mask=torch.zeros((src_seq_len,src_seq_len),device=device).type(torch.bool)\n",
        "memory=transformer.encode(src,src_mask)\n",
        "ys=torch.ones(1,1).fill_(bos_idx).type(torch.long).to(device)\n",
        "num_tokens=src.shape[0]\n",
        "max_len=num_tokens+5\n",
        "memory=memory.to(device)\n",
        "\n",
        "for j in range(max_len-1):\n",
        "  tgt_mask=(generate_square_mask(ys.size(0))).type(torch.bool).to(device)\n",
        "  out=transformer.decode(ys,memory,tgt_mask)\n",
        "  out=out.transpose(0,1) \n",
        "  prob=transformer.generator(out[:,-1])\n",
        "  _, next_char=torch.max(prob,dim=1)\n",
        "  next_char=next_char.item()\n",
        "\n",
        "  ys=torch.cat([ys,torch.ones(1,1).type_as(src.data).fill_(next_char)],dim=0) \n",
        "  if next_char==eos_idx:\n",
        "    break\n",
        "\n",
        "ys=ys.flatten().cpu().numpy()\n",
        "output_name=\"\".join([output_lang.index2char[idx] for idx in ys]).replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
        "#tgt_name=\"\".join([char for char in tgt_name]).replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
        "      #output_name=output_name.split(\" \")\n",
        "\n",
        "output_name=output_name.split(\" \")\n",
        "#tgt_name=tgt_name.split(\" \")\n",
        "print(\"Source name: \", src_name)\n",
        "print(\"Prediction: \", output_name)\n",
        "#print(\"Target: \", tgt_name) \n",
        "print(\"Time taken: \",timming(start))\n"
      ]
    }
  ]
}