{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oe9THx-mphxQ",
        "outputId": "768877ce-602c-4ca1-b51e-01566daab4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading lines...\n",
            "Read 401044 name pairs\n",
            "Counting characters:\n",
            "counted characters:\n",
            "En 164\n",
            "Rus 39\n",
            "[1] Started fold:\n",
            "[9262, 7969, 6469, 3448, 9546]\n",
            "Fold loss: 3.429 \t Avg fold loss: 3.737 \t time for fold is 0m 7s\n",
            "Training completion: 0.31% \t remiaing indexes in training: 8945\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[1900, 4443, 76, 3248, 864]\n",
            "Fold loss: 3.563 \t Avg fold loss: 3.531 \t time for fold is 0m 14s\n",
            "Training completion: 0.58% \t remiaing indexes in training: 8895\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[2813, 5757, 3357, 4494, 4728]\n",
            "Fold loss: 3.294 \t Avg fold loss: 3.450 \t time for fold is 0m 20s\n",
            "Training completion: 0.86% \t remiaing indexes in training: 8845\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[3323, 5030, 8016, 4492, 9979]\n",
            "Fold loss: 3.209 \t Avg fold loss: 3.397 \t time for fold is 0m 26s\n",
            "Training completion: 1.14% \t remiaing indexes in training: 8795\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[1655, 5615, 6330, 664, 7393]\n",
            "Fold loss: 3.130 \t Avg fold loss: 3.360 \t time for fold is 0m 33s\n",
            "Training completion: 1.42% \t remiaing indexes in training: 8745\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[5747, 4690, 8912, 242, 1115]\n",
            "Fold loss: 2.964 \t Avg fold loss: 3.314 \t time for fold is 0m 39s\n",
            "Training completion: 1.69% \t remiaing indexes in training: 8695\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[3636, 4, 5384, 7730, 7130]\n",
            "Fold loss: 2.934 \t Avg fold loss: 3.286 \t time for fold is 0m 46s\n",
            "Training completion: 1.97% \t remiaing indexes in training: 8645\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[811, 6083, 3703, 1036, 1470]\n",
            "Fold loss: 2.856 \t Avg fold loss: 3.248 \t time for fold is 0m 52s\n",
            "Training completion: 2.25% \t remiaing indexes in training: 8595\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[3997, 5681, 3629, 7892, 9789]\n",
            "Fold loss: 2.847 \t Avg fold loss: 3.206 \t time for fold is 1m 0s\n",
            "Training completion: 2.53% \t remiaing indexes in training: 8545\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[6223, 2717, 9731, 7917, 5246]\n",
            "Fold loss: 2.811 \t Avg fold loss: 3.179 \t time for fold is 1m 6s\n",
            "Training completion: 2.81% \t remiaing indexes in training: 8495\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[6523, 2259, 5177, 8761, 7956]\n",
            "Fold loss: 2.794 \t Avg fold loss: 3.149 \t time for fold is 1m 13s\n",
            "Training completion: 3.08% \t remiaing indexes in training: 8445\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[683, 4741, 4546, 9173, 322]\n",
            "Fold loss: 2.595 \t Avg fold loss: 3.108 \t time for fold is 1m 19s\n",
            "Training completion: 3.36% \t remiaing indexes in training: 8395\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[5045, 6197, 9869, 3760, 9887]\n",
            "Fold loss: 3.180 \t Avg fold loss: 3.073 \t time for fold is 1m 26s\n",
            "Training completion: 3.64% \t remiaing indexes in training: 8345\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[7913, 5403, 5981, 8095, 4973]\n",
            "Fold loss: 2.466 \t Avg fold loss: 3.040 \t time for fold is 1m 32s\n",
            "Training completion: 3.92% \t remiaing indexes in training: 8295\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[1683, 6900, 7298, 871, 4008]\n",
            "Fold loss: 2.352 \t Avg fold loss: 3.008 \t time for fold is 1m 39s\n",
            "Training completion: 4.19% \t remiaing indexes in training: 8245\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[9433, 9646, 9311, 7157, 7086]\n",
            "Fold loss: 2.769 \t Avg fold loss: 2.982 \t time for fold is 1m 45s\n",
            "Training completion: 4.47% \t remiaing indexes in training: 8195\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[1792, 32, 332, 3184, 8733]\n",
            "Fold loss: 2.599 \t Avg fold loss: 2.951 \t time for fold is 1m 55s\n",
            "Training completion: 4.75% \t remiaing indexes in training: 8145\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[640, 9520, 223, 3947, 4943]\n",
            "Fold loss: 2.414 \t Avg fold loss: 2.925 \t time for fold is 2m 3s\n",
            "Training completion: 5.03% \t remiaing indexes in training: 8095\n",
            "--------------------------------------------\n",
            "[1] Started fold:\n",
            "[9283, 4458, 1970, 7460, 3103]\n",
            "Fold loss: 2.505 \t Avg fold loss: 2.904 \t time for fold is 2m 9s\n",
            "Training completion: 5.31% \t remiaing indexes in training: 8045\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4371a17b6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mtgt_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f4371a17b6d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msrc_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtgt_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_tok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    143\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    348\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                            need_weights=False)[0]\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4986\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4987\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4988\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4989\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4990\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4737\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4738\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4740\u001b[0m             \u001b[0;31m# encoder-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division \n",
        "from io import open \n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import unicodedata \n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "#plt.switch_backend(\"agg\")\n",
        "import matplotlib.ticker as ticker \n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "#!pip install thai_tokenizer \n",
        "#from thai_tokenizer import Tokenizer\n",
        "#from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(123456789)\n",
        "\n",
        "epochs=2\n",
        "epoch=1\n",
        "nr_pairs_subset=10**(4) \n",
        "warmup_steps=10**(2)\n",
        "#nr_pairs_subset=len(pairs) \n",
        "emb_size=512\n",
        "nhead=8\n",
        "#ffn_hid_dim=256\n",
        "#batch_size=32\n",
        "num_encoder_layers=4\n",
        "num_decoder_layers=4\n",
        "\n",
        "file1_text = \"/content/drive/MyDrive/WunderSchildt/Translation/Prints/Char rus CV transformer, epoch %d, names %d,embedding %d.txt\" % (epochs,nr_pairs_subset,emb_size)\n",
        "file1 = open(file1_text, \"w\")\n",
        "\n",
        "\n",
        "##############3\n",
        "#Functions that transform the words in numbers. Needs a tokenizer since in certain languages \n",
        "#1 english word=n other language words \n",
        "###############\n",
        "\n",
        "src_lang=\"En\"\n",
        "tgt_lang=\"Rus\"\n",
        "\n",
        "\n",
        "unk_idx=0\n",
        "pad_idx=1\n",
        "bos_idx=2\n",
        "eos_idx=3\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,emb_size:int,dropout:float,maxlen=5000):\n",
        "        super().__init__()\n",
        "        den=torch.exp(-torch.arange(0,emb_size,2)*math.log(10000)/emb_size)\n",
        "        pos=torch.arange(0,maxlen).reshape(maxlen,1)\n",
        "        pos_embedding=torch.zeros((maxlen,emb_size))\n",
        "        pos_embedding[:,0::2]=torch.sin(pos*den)\n",
        "        pos_embedding[:,1::2]=torch.cos(pos*den)\n",
        "        pos_embedding=pos_embedding.unsqueeze(-2)\n",
        "        \n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.register_buffer(\"pos_embedding\",pos_embedding)\n",
        "        \n",
        "    def forward(self,token_embedding):\n",
        "        return self.dropout(token_embedding+self.pos_embedding[:token_embedding.size(0),:])\n",
        "        \n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_size):\n",
        "        super().__init__()\n",
        "        self.embedding=nn.Embedding(vocab_size,emb_size)\n",
        "        self.emb_size=emb_size\n",
        "        \n",
        "    def forward(self,tokens):\n",
        "        return self.embedding(tokens.long())*math.sqrt(self.emb_size)\n",
        "        \n",
        "        \n",
        "class seq2seqTransformer(nn.Module):\n",
        "    def __init__(self,num_encoder_layers,num_decoder_layers,emb_size,nhead,src_vocab_size,tgt_vocab_size,\n",
        "                dim_feedforward=512,dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer=nn.Transformer(d_model=emb_size,nhead=nhead,num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,dropout=dropout)\n",
        "        \n",
        "        self.generator=nn.Linear(emb_size,tgt_vocab_size)\n",
        "        self.src_tok_emb=TokenEmbedding(src_vocab_size,emb_size)\n",
        "        self.tgt_tok_emb=TokenEmbedding(tgt_vocab_size,emb_size)\n",
        "        self.positional_encoding=PositionalEncoding(emb_size,dropout)\n",
        "        \n",
        "    def forward(self,src,tgt,src_mask,tgt_mask,src_padding_mask,tgt_padding_mask,memory_key_padding_mask):\n",
        "        src_emb=self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb=self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        out=self.transformer(src_emb,tgt_emb,src_mask,tgt_mask,None,src_padding_mask,tgt_padding_mask,memory_key_padding_mask) \n",
        "        out=self.generator(out)\n",
        "        return out\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)),src_mask)\n",
        "    \n",
        "    def decode(self,tgt,memory,tgt_mask):\n",
        "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)),memory,tgt_mask)\n",
        "        \n",
        "\n",
        "def generate_square_mask(size):\n",
        "    mask=(torch.triu(torch.ones((size,size),device=device))==1).transpose(0,1)   #make lower triangular with 1's\n",
        "    #make suare matrix in upper triangle has -inf and in the lower triangle it has 0.0\n",
        "    mask=mask.float().masked_fill(mask==0,float(\"-inf\")).masked_fill(mask==1,float(0.0))   \n",
        "    return mask\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self,lang):\n",
        "        self.lang=lang\n",
        "        self.char2index={}\n",
        "        self.char2count={}\n",
        "        self.index2char={0: \"<unk>\", 1:\"<pad>\", 2:\"<bos>\", 3:\"<eos>\" }\n",
        "        self.n_chars=4\n",
        "        \n",
        "    def addName(self,sentence):\n",
        "        for char in sentence:\n",
        "            self.addChar(char.lower())\n",
        "            \n",
        "    def addChar(self, char):\n",
        "        if char not in self.char2index:\n",
        "            self.char2index[char]=self.n_chars\n",
        "            self.char2count[char]=1\n",
        "            self.index2char[self.n_chars]=char\n",
        "            self.n_chars+=1\n",
        "        else:\n",
        "            self.char2count[char]+=1 \n",
        "            \n",
        "token_transform = {}\n",
        "def timming(since):\n",
        "    now=time.time()\n",
        "    s=now-since\n",
        "    m=int(s/60)\n",
        "    s=s-m*60\n",
        "    return \"%dm %ds\"%(m,s)\n",
        "def unicodeToAscii(s):\n",
        "        return ''.join(c for c in unicodedata.normalize(\"NFD\",s) if unicodedata.category(c)!=\"Mn\")\n",
        "def normalizeString(string):\n",
        "    string=unicodeToAscii(string.lower().strip())\n",
        "    string=re.sub(r\"([.!?])\",r\" \\1\",string)\n",
        "    string=string.replace(\",\",\"\")\n",
        "    return string\n",
        "\n",
        "def readLangs(src_lang,tgt_lang,reverse):\n",
        "    \n",
        "    forbidden_chars=['\\u200e','\\u200b','\\u200f','ª',')','|','[',']','ǃ','$','‐','?','+','̱','̄','̜','̕','︡', '︠','̈',\"ʿ\",'\"',\"ʹ\",\"ʼ\",\"‘\",\"′\",\"'́'\",\"«\",\"»\",\"\\xad\",\"\\\\\",\".\"]\n",
        "    apostrophes=[\"`\",\"’\",\"ʻ\",\"ʾ\",\"'\"]\n",
        "    chirilic=[\"А\", \"Б\", \"В\", \"Г\", \"Д\", \"Е\", \"Ё\", \"Ж\", \"З\", \"И\", \"Й\", \"К\", \"Л\", \"М\", \"Н\", \"О\", \"П\", \"Р\", \"С\", \"Т\", \"У\", \"Ф\", \"Х\", \"Ц\", \"Ч\", \"Ш\", \"Щ\", \"Ъ\", \n",
        "              \"Ы\", \"Ь\", \"Э\", \"Ю\", \"Я\"]\n",
        "    chirilic=[char.lower() for char in chirilic]\n",
        "    sh=[\"ş\",\"ṣ\",\"ș\"]\n",
        "\n",
        "    print(\"reading lines...\")\n",
        "    file1.write(\"reading lines... \\n\")\n",
        "    \n",
        "    lines=open(\"/content/drive/MyDrive/WunderSchildt/Translation/Data/%s-%s Names.txt\"%(src_lang,tgt_lang),encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
        "    #lines=open(\"./Data/%s-%s_Names.txt\"%(src_lang,tgt_lang),encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
        "    pairs=[]\n",
        "    correct_line_idx=[] \n",
        "    i=0\n",
        "    while i<=len(lines)-1:\n",
        "      flag=1\n",
        "      #Keep turkish, etc names or not?\n",
        "      #lines[i]=unicodeToAscii(lines[i])\n",
        "      if \"-\" in lines[i]:\n",
        "        lines[i]=lines[i].replace(\"-\",\"-\")\n",
        "      if \"–\" in lines[i]:\n",
        "        lines[i]=lines[i].replace(\"–\",\"-\")\n",
        "      for forbidden_char in forbidden_chars:\n",
        "        if forbidden_char in lines[i]:\n",
        "          lines[i]=lines[i].replace(forbidden_char,\"\")\n",
        "      for apos in apostrophes:\n",
        "        if apos in lines[i]:\n",
        "          lines[i]=lines[i].replace(apos,\"'\") \n",
        "      for s in sh:\n",
        "        if s in lines[i]:\n",
        "          lines[i]=lines[i].replace(s,\"ş\")\n",
        "      \n",
        "      if reverse==False:\n",
        "        s_input=lines[i].split(\"\\t\")[0]\n",
        "        s_output=lines[i].split(\"\\t\")[1]\n",
        "        temp_output=s_output.replace(\" \",\"\").replace(\",\",\"\").replace(\"-\",\"\")\n",
        "        temp_output=[char.lower() for char in temp_output]\n",
        "        for char in temp_output:\n",
        "          if char not in chirilic:\n",
        "            flag=0\n",
        "            break\n",
        "      else:\n",
        "        s_output=lines[i].split(\"\\t\")[0]\n",
        "        s_input=lines[i].split(\"\\t\")[1]\n",
        "        temp_input=s_input.replace(\" \",\"\").replace(\",\",\"\").replace(\"-\",\"\")\n",
        "        temp_input=[char.lower() for char in temp_input]\n",
        "        for char in temp_input:\n",
        "          if char not in chirilic:\n",
        "            flag=0\n",
        "            break\n",
        "\n",
        "      if flag==0:\n",
        "        i+=1\n",
        "      else:\n",
        "        correct_line_idx.append(i)\n",
        "        if \",\" in lines[i]:\n",
        "          if \",\" in s_output:\n",
        "            temp=s_output.split(\",\")\n",
        "            temp=[word.replace(\" \",\"\") for word in temp]\n",
        "            temp=[temp[len(temp)-i] for i in range(1,len(temp)+1)]\n",
        "            s_output=\" \".join(temp)\n",
        "            s_output=[char.lower() for char in s_output]\n",
        "          if \",\" in s_input:\n",
        "            temp=s_input.split(\",\")\n",
        "            temp=[word.replace(\" \",\"\") for word in temp]\n",
        "            temp=[temp[len(temp)-i] for i in range(1,len(temp)+1)]\n",
        "            s_input=\" \".join(temp) \n",
        "            s_input=[char.lower() for char in s_input]    \n",
        "          pairs.append([s_input,s_output])\n",
        "        else: \n",
        "          s_input=[char.lower() for char in s_input]\n",
        "          s_output=[char.lower() for char in s_output]\n",
        "          pairs.append([s_input,s_output])\n",
        "        i+=1\n",
        "\n",
        "    if reverse==False:\n",
        "        input_lang=Lang(src_lang)\n",
        "        output_lang=Lang(tgt_lang)\n",
        "    else:\n",
        "        #pairs=[list(reversed(p)) for p in pairs]\n",
        "        input_lang=Lang(tgt_lang)\n",
        "        output_lang=Lang(src_lang) \n",
        "        \n",
        "    return input_lang, output_lang, pairs, correct_line_idx\n",
        "\n",
        "\n",
        "def prepareData(src_lang,tgt_lang,reverse):\n",
        "    input_lang, output_lang, pairs, correct_line_idx=readLangs(src_lang,tgt_lang,reverse)\n",
        "    #token_transform[src_lang] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "    #token_transform[tgt_lang] = get_tokenizer('spacy', language='rus')\n",
        "    print(\"Read %s name pairs\"%len(pairs))\n",
        "    file1.write(\"Read %s name pairs \\n\"%len(pairs))\n",
        "    print(\"Counting characters:\")\n",
        "    file1.write(\"Counting characters: \\n\")\n",
        "    for pair in pairs:      \n",
        "        input_lang.addName(pair[0])\n",
        "        output_lang.addName(pair[1]) \n",
        "        \n",
        "    print(\"counted characters:\")\n",
        "    file1.write(\"counted characters: \\n\")\n",
        "    print(input_lang.lang,input_lang.n_chars)\n",
        "    file1.write(str(input_lang.lang)+\":\"+str(input_lang.n_chars)+\"\\n\") \n",
        "    print(output_lang.lang,output_lang.n_chars) \n",
        "    file1.write(str(output_lang.lang)+\":\"+str(output_lang.n_chars)+\"\\n\") \n",
        "    \n",
        "    return input_lang, output_lang, pairs, correct_line_idx\n",
        "\n",
        "def create_mask(src,tgt):\n",
        "    src_seq_len=src.shape[0]\n",
        "    tgt_seq_len=tgt.shape[0]\n",
        "    \n",
        "    tgt_mask=generate_square_mask(tgt_seq_len)\n",
        "    src_mask=torch.zeros((src_seq_len,src_seq_len),device=device).type(torch.bool)\n",
        "    \n",
        "    src_padding_mask=(src==pad_idx).transpose(0,1)\n",
        "    tgt_padding_mask=(tgt==pad_idx).transpose(0,1)\n",
        "    \n",
        "    return src_mask,tgt_mask,src_padding_mask,tgt_padding_mask   \n",
        "\n",
        "\n",
        "def indexesFromSentence(lang,sentence):\n",
        "    return [lang.char2index[char] for char in sentence]\n",
        "\n",
        "def tensorFromSentence(lang,sentence):\n",
        "    indexes=indexesFromSentence(lang,sentence)\n",
        "    indexes.append(eos_idx)\n",
        "    indexes.insert(0,bos_idx)\n",
        "    return torch.tensor(indexes,dtype=torch.long,device=device).view(-1,1)\n",
        "\n",
        "def collate_fn(pair):\n",
        "    src_batch, tgt_batch = [], [] \n",
        "    src_list=[input_lang.char2index[char.lower()] for char in pair[0]]\n",
        "    src_list.insert(0,bos_idx)\n",
        "    src_list.append(eos_idx)\n",
        "        \n",
        "    tgt_list=[output_lang.char2index[char.lower()] for char in pair[1]]\n",
        "    tgt_list.insert(0,bos_idx)\n",
        "    tgt_list.append(eos_idx)\n",
        "        \n",
        "    src_batch.append(torch.tensor(src_list,dtype=torch.long,device=device).view(-1,1))\n",
        "    tgt_batch.append(torch.tensor(tgt_list,dtype=torch.long,device=device).view(-1,1))\n",
        "        \n",
        "    src_batch=pad_sequence(src_batch,padding_value=pad_idx)\n",
        "    tgt_batch=pad_sequence(tgt_batch,padding_value=pad_idx)\n",
        "    \n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs, correct_line_idx=prepareData(src_lang,tgt_lang,reverse=False)\n",
        "\n",
        "nr_pairs_subset=10**(4) \n",
        "#nr_pairs_subset=len(pairs) \n",
        "pairs_subset_idx=np.random.choice(range(len(pairs)),size=nr_pairs_subset,replace=True)\n",
        "pairs_subset=[pairs[idx] for idx in pairs_subset_idx] \n",
        "\n",
        "percentage_train=0.9\n",
        "training_idx=np.random.choice(range(nr_pairs_subset),size=int(nr_pairs_subset*percentage_train),replace=False)\n",
        "testing_idx=[idx for idx in range(nr_pairs_subset) if idx not in training_idx]\n",
        "\n",
        "pairs_train=[pairs_subset[idx] for idx in training_idx]\n",
        "pairs_test=[pairs_subset[idx] for idx in testing_idx]\n",
        "nr_test=len(pairs_test)\n",
        "nr_train=len(pairs_train)\n",
        "\n",
        "src_vocab_size=input_lang.n_chars\n",
        "tgt_vocab_size=output_lang.n_chars\n",
        "emb_size=512\n",
        "nhead=8\n",
        "#ffn_hid_dim=256\n",
        "batch_size=32\n",
        "num_encoder_layers=4\n",
        "num_decoder_layers=4\n",
        "\n",
        "\n",
        "transformer=seq2seqTransformer(num_encoder_layers,num_decoder_layers,emb_size,nhead,src_vocab_size,tgt_vocab_size,\n",
        "                                dim_feedforward=512,dropout=0.1)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim()>1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "        \n",
        "transformer=transformer.to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "#lr=10**(-4)\n",
        "lr=0\n",
        "optimizer=torch.optim.Adam(transformer.parameters(),lr=lr,betas=(0.9,0.98),eps=10**(-9),amsgrad=True)\n",
        "epochs=2\n",
        "epoch=1\n",
        "printing_iter=10**(2) \n",
        "\n",
        "all_folds_idx=[]\n",
        "train_losses=[]\n",
        "k_folds=5\n",
        "overall_accuracy=[]\n",
        "count_train=0\n",
        "\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "    rem_data_indexes=training_idx\n",
        "    flag=1\n",
        "    start=time.time()\n",
        "    while flag==1:\n",
        "      nr_data=len(rem_data_indexes)\n",
        "      if nr_data<=k_folds and nr_data>0:\n",
        "        folds_idx=rem_data_indexes\n",
        "        #rem_data_indexes=folds_idx\n",
        "        flag=0\n",
        "      if nr_data>k_folds:\n",
        "        folds_idx=np.random.choice(rem_data_indexes,size=k_folds,replace=False).tolist()\n",
        "        rem_data_indexes=[idx for idx in rem_data_indexes if idx not in folds_idx]\n",
        "      \n",
        "      fold_loss=0\n",
        "      for i in folds_idx:\n",
        "        src, tgt=collate_fn(pairs[i])\n",
        "        optimizer=torch.optim.Adam(transformer.parameters(),lr=lr,betas=(0.9,0.98),eps=10**(-9))\n",
        "        count_train+=1\n",
        "        step_num=count_train\n",
        "        lr=1/math.sqrt(emb_size)*min(1/math.sqrt(step_num+1),(step_num+1)*warmup_steps**(-1.5))\n",
        "        optimizer.zero_grad() \n",
        "        train_loss=0\n",
        "        src=src[:,0,:].to(device)\n",
        "        tgt=tgt[:,0,:].to(device)\n",
        "        \n",
        "        tgt_input=tgt[:-1,:]\n",
        "        \n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask=create_mask(src,tgt_input)\n",
        "        \n",
        "        output=transformer(src,tgt_input,src_mask,tgt_mask,src_padding_mask,tgt_padding_mask,src_padding_mask)\n",
        "        \n",
        "        tgt_out=tgt[1:,:]\n",
        "        tgt_out=tgt_out.reshape(-1)\n",
        "        output=output.reshape(-1,output.shape[-1])\n",
        "        fold_loss+=criterion(output,tgt_out) \n",
        "\n",
        "      fold_loss.backward()\n",
        "      optimizer.step()\n",
        "        \n",
        "      train_losses.append(fold_loss.item()/len(folds_idx))\n",
        "\n",
        "      if (count_train%printing_iter==0 and count_train>0):\n",
        "        print(\"[%d] Started fold:\"%(epoch))\n",
        "        file1.write(\"[%d] Started fold:\\n\"%(epoch))\n",
        "        print(folds_idx)\n",
        "        file1.write(\"Folds are: \" + \",\".join([str(idx) for idx in folds_idx])+\"\\n\") \n",
        "        print(\"Fold loss: %.3f \\t Avg fold loss: %.3f \\t time for fold is %s\"%(fold_loss/len(folds_idx),np.mean(train_losses),timming(start)))\n",
        "        file1.write(\"Fold loss: %.3f \\t Avg fold loss: %.3f \\t time for fold is %s \\n\"%(fold_loss/len(folds_idx),np.mean(train_losses),timming(start)))\n",
        "        print(\"Training completion: %.2f%% \\t nr of remiaing indexes in training: %d\"%((100*epoch*(nr_train-len(rem_data_indexes)))/(nr_train*epochs),len(rem_data_indexes)))\n",
        "        file1.write(\"Training completion: %.2f%% \\t nr of remiaing indexes in training: %d\"%((100*epoch*(nr_train-len(rem_data_indexes)))/(nr_train*epochs),len(rem_data_indexes)))\n",
        "        print(\"--------------------------------------------\")\n",
        "        file1.write(\"--------------------------------------------\\n\")\n",
        "\n",
        "    accuracy=[]\n",
        "    input_names=[]\n",
        "    output_names=[]\n",
        "    target_names=[]\n",
        "    correct_predicted_names=[]\n",
        "    correct_actual_names=[]\n",
        "    correct_latin_names=[]\n",
        "    incorrect_predicted_names=[]\n",
        "    incorrect_actual_names=[]\n",
        "    incorrect_latin_names=[]\n",
        "    count_test=0\n",
        "    u=0\n",
        "    with torch.no_grad():\n",
        "      rem_data_indexes=testing_idx\n",
        "      flag=1\n",
        "      while flag==1:\n",
        "        nr_data=len(rem_data_indexes)\n",
        "        if nr_data<=k_folds and nr_data>0:\n",
        "          folds_idx=rem_data_indexes\n",
        "          #rem_data_indexes=folds_idx\n",
        "          flag=0\n",
        "        if nr_data>k_folds:\n",
        "          folds_idx=np.random.choice(rem_data_indexes,size=k_folds,replace=False).tolist()\n",
        "          rem_data_indexes=[idx for idx in rem_data_indexes if idx not in folds_idx]\n",
        "\n",
        "        for i in folds_idx:\n",
        "          src_name=pairs[i][0]\n",
        "          tgt_name=pairs[i][1]\n",
        "          src_list=[input_lang.char2index[char.lower()] for char in src_name]\n",
        "          src_list.insert(0,bos_idx)\n",
        "          src_list.append(eos_idx)\n",
        "          src=torch.tensor(src_list,dtype=torch.long,device=device).view(-1,1)\n",
        "          src=src.view(src.shape[0],1,1)\n",
        "          #src=pad_sequence(src,padding_value=pad_idx)\n",
        "\n",
        "          src=src[:,0].to(device)\n",
        "          src_seq_len=src.shape[0]\n",
        "          src_mask=torch.zeros((src_seq_len,src_seq_len),device=device).type(torch.bool)\n",
        "          memory=transformer.encode(src,src_mask)\n",
        "          ys=torch.ones(1,1).fill_(bos_idx).type(torch.long).to(device)\n",
        "          num_tokens=src.shape[0]\n",
        "          max_len=num_tokens+5\n",
        "          memory=memory.to(device)\n",
        "\n",
        "          for j in range(max_len-1):\n",
        "            tgt_mask=(generate_square_mask(ys.size(0))).type(torch.bool).to(device)\n",
        "            out=transformer.decode(ys,memory,tgt_mask)\n",
        "            out=out.transpose(0,1) \n",
        "            out=transformer.generator(out[:,-1])\n",
        "            prob=F.softmax(out,dim=1)\n",
        "            _, next_char=torch.max(prob,dim=1)\n",
        "            next_char=next_char.item()\n",
        "\n",
        "            ys=torch.cat([ys,torch.ones(1,1).type_as(src.data).fill_(next_char)],dim=0) \n",
        "            if next_char==eos_idx:\n",
        "              break\n",
        "\n",
        "          ys=ys.flatten().cpu().numpy()\n",
        "          output_name=\"\".join([output_lang.index2char[idx] for idx in ys]).replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
        "          tgt_name=\"\".join([char for char in tgt_name]).replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
        "          #output_name=output_name.split(\" \")\n",
        "\n",
        "          output_name=output_name.split(\" \")\n",
        "          tgt_name=tgt_name.split(\" \")\n",
        "          output_names.append(output_name)\n",
        "          target_names.append(tgt_name)\n",
        "          input_names.append(src_name)\n",
        "          \n",
        "        count_test+=1\n",
        "\n",
        "        if output_name==tgt_name:\n",
        "          u+=1\n",
        "          temp=\" \".join([name for name in input_names[-1]])\n",
        "          correct_latin_names.append(temp)\n",
        "          temp=\" \".join([name for name in output_names[-1]])\n",
        "          correct_predicted_names.append(temp)\n",
        "          temp=\" \".join([name for name in target_names[-1]])\n",
        "          correct_actual_names.append(temp)\n",
        "        else: \n",
        "          temp=\" \".join([name for name in input_names[-1]])\n",
        "          incorrect_latin_names.append(temp)\n",
        "          temp=\" \".join([name for name in output_names[-1]])\n",
        "          incorrect_predicted_names.append(temp)\n",
        "          temp=\" \".join([name for name in target_names[-1]])\n",
        "          incorrect_actual_names.append(temp)\n",
        "\n",
        "        overall_accuracy.append(100*u/(i+1))\n",
        "\n",
        "        if (count_test%printing_iter==0 and count_test>0):\n",
        "          \n",
        "          print(\"[%.2f%%] Testing completion:%.2f%% \\t Accuracy avg so far:%.5f%% \\t time:%s\"%\n",
        "                (100*epoch/epochs, 100*epoch*(nr_test-len(rem_data_indexes))/(nr_test*epochs), overall_accuracy[-1], timming(start)))\n",
        "          file1.write(\"[%.2f%%] Testing completion:%.2f%% \\t Accuracy avg so far:%.5f%% \\t time:%s \\n\"%\n",
        "                (100*epoch/epochs, 100*epoch*(nr_test-len(rem_data_indexes))/(nr_test*epochs), overall_accuracy[-1], timming(start)))\n",
        "          print(\"--------------------------------------------\")\n",
        "          file1.write(\"-------------------------------------------- \\n\") \n",
        "\n",
        "\n",
        "    print(\"[%.2f%%] Training loss=%.7f \\t Testing mean accuracy:%.2f%% \\t time=%s\"%(100*epoch/epochs,train_losses[-1],overall_accuracy[-1],timming(start)))\n",
        "    file1.write(\"[%.2f%%] Training loss=%.7f \\t Testing mean accuracy:%.2f%% \\t time=%s \\n\"%(100*epoch/epochs,train_losses[-1],overall_accuracy[-1],timming(start)))\n",
        "\n",
        "    torch.save(transformer,\"/content/drive/MyDrive/WunderSchildt/Translation/All_saved_models/Char CV rus transformer, epoch %d, names %d, folds %d,embedding %d, loss %.4f, layers %d, acc %.2f%%.pth\"%\n",
        "           (epoch,nr_pairs_subset,k_folds,emb_size,np.mean(train_losses),num_encoder_layers,100*u/(count_test*(len(folds_idx)))))\n",
        "    \n",
        "    correct_names=pd.DataFrame()\n",
        "    correct_names[\"correct predicted names\"]=correct_predicted_names\n",
        "    correct_names[\"actual names\"]=correct_actual_names\n",
        "    correct_names[\"latin names\"]=correct_latin_names\n",
        "    correct_names.to_csv(\"/content/drive/MyDrive/WunderSchildt/Translation/Predictions/Translated Correct Names by char CV rus transformer, epoch %d, names %d, folds %d,embedding %d, loss %.4f, layers %d, acc %.2f%%.csv\"%\n",
        "                  (epoch,nr_pairs_subset,k_folds,emb_size,np.mean(train_losses),num_encoder_layers,overall_accuracy[-1]))\n",
        "    \n",
        "    incorrect_names=pd.DataFrame()\n",
        "    incorrect_names[\"incorrect predicted names\"]=incorrect_predicted_names\n",
        "    incorrect_names[\"actual names\"]=incorrect_actual_names\n",
        "    incorrect_names[\"latin names\"]=incorrect_latin_names\n",
        "    incorrect_names.to_csv(\"/content/drive/MyDrive/WunderSchildt/Translation/Predictions/Translated Incorrect Names by char CV rus transformer, epoch %d, names %d,folds %d,embedding %d, loss %.4f, layers %d, acc %.2f%%.csv\"%\n",
        "                  (epoch,nr_pairs_subset,k_folds,emb_size,np.mean(train_losses),num_encoder_layers,overall_accuracy[-1]))\n",
        "\n",
        "      \n",
        "file1.close()\n",
        "plt.title(\"Training loss\")\n",
        "plt.plot(train_losses,label=\"loss\")\n",
        "plt.xlabel(\"Number of names\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend() \n",
        "plt.show()\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(overall_accuracy,label=\"accuracy\")\n",
        "plt.xlabel(\"Number of names\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "train_losses_avg=[sum(train_losses[:(i+1)])/(i+1) for i in range(len(train_losses)-1)]\n",
        "plt.title(\"Training loss avg\")\n",
        "plt.plot(train_losses_avg,label=\"avg loss\")\n",
        "plt.xlabel(\"Number of names\")\n",
        "plt.ylabel(\"Avg Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "char_rus_transformer_cv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}